{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49c44fe-bfb8-4b38-9e6e-027db2892153",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering Recency, Frequency, and Monetary (RFM) individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa3c3b-fd90-4308-99e6-a1b5e32e8d36",
   "metadata": {
    "tags": []
   },
   "source": [
    "We're going to explore clustering recency, frequency, and monetary value indivudually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d6719a-3a08-4a34-9532-96e4bc21f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d5d29-bfe1-4a05-8fda-9b11a3fc795b",
   "metadata": {},
   "source": [
    "### RECENCY, FREQUENCY, MONETARY ANALYSIS \n",
    "Objective is to find the RFM for each customer to determing Low, Medium and High Value Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913809bf-e597-4b8d-958c-727511a3db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/home/jupyter/data_science_challenges/business_data_science'\n",
    "\n",
    "data_dir = os.path.join(home_dir, 'data/external/OnlineRetail.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6749bf-8253-4a2c-8491-217732d0bb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir, encoding=\"unicode_escape\").drop_duplicates().dropna().reset_index(drop=True)\n",
    "\n",
    "# Changing data types\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "# time series variables\n",
    "df['date'] = pd.to_datetime(df.InvoiceDate).dt.date\n",
    "df['year'] = pd.to_datetime(df.InvoiceDate).dt.year\n",
    "df['month'] = pd.to_datetime(df.InvoiceDate).dt.month\n",
    "df['day'] = pd.to_datetime(df.InvoiceDate).dt.day\n",
    "df['yearmo'] = pd.to_datetime(df.InvoiceDate).dt.strftime('%Y%m')\n",
    "# new variable\n",
    "\n",
    "df['Revenue'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "df = df.loc[df.Country == 'United Kingdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d73f7e-8a23-4570-85d4-c08524f88548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop returns.\n",
    "new_df = df.loc[~df.InvoiceNo.str.contains('C')].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33837f-37f2-4120-8cb9-954698ea7aba",
   "metadata": {},
   "source": [
    "#### Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24d2a94-6993-4ca1-b485-c5e77fc9e371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = df.loc[~df.InvoiceNo.str.contains('C')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29276c4a-79b7-44f7-b0c1-d4c224be576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop returns.\n",
    "new_df = df.loc[~df.InvoiceNo.str.contains('C')].copy()\n",
    "new_df['Recency'] =  (pd.to_datetime(new_df.InvoiceDate).max() - new_df.InvoiceDate).dt.days \n",
    "r_df = new_df.groupby(['CustomerID']).agg({'InvoiceDate': 'min'}).reset_index()\\\n",
    ".merge(new_df[['CustomerID', 'InvoiceDate','Recency']], on=['CustomerID','InvoiceDate'], how='inner')\\\n",
    ".drop_duplicates().reset_index(drop=True).drop(['InvoiceDate'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee443d4-a825-4d81-a071-6186aad81498",
   "metadata": {},
   "source": [
    "#### Frequency\n",
    "We are going to use the formula:\n",
    "Frequency = 1 / Median( Time between Purchases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b96866-15da-4d46-bd73-33fc00fe76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.loc[~df.InvoiceNo.str.contains('C')].copy()\n",
    "f_df = new_df[['InvoiceDate','CustomerID']].drop_duplicates().sort_values(by=['CustomerID','InvoiceDate'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "#Creating difference between invoice dates per person\n",
    "f_df['Offset'] = f_df.groupby(['CustomerID'])['InvoiceDate'].diff().dt.seconds\n",
    "\n",
    "# Creating Count per user\n",
    "f_df = f_df.merge(\\\n",
    "    f_df.groupby(['CustomerID']).agg(Count = ('InvoiceDate', 'count')).reset_index()\\\n",
    "           ,on=['CustomerID'], how='inner')\n",
    "\n",
    "# Finding folks who only had one interaction and giving them an Offset that is differenced from the latest day in the dataset\n",
    "f_df.loc[f_df.Count == 1, 'Offset'] = (f_df.InvoiceDate.max() - f_df.InvoiceDate).dt.seconds\n",
    "\n",
    "# Dropping the first occurance of a invoicedate.  Not needed to make agg.\n",
    "f_df = f_df.loc[~(f_df.Offset.isna())]\n",
    "\n",
    "f_df = f_df.groupby(['CustomerID']).agg(Frequency = ('Offset','median')).reset_index()\n",
    "\n",
    "#f_df['FrequencyHertz'] = 1/f_df.Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee17ed-0e69-4687-9b14-fda099f9bedf",
   "metadata": {},
   "source": [
    "#### Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b29ce2-7ad0-4dac-a59e-9beb17a0ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use DF instead of new_df here because we want to penalize users for their returns.\n",
    "m_df = df[['CustomerID','Revenue']].groupby('CustomerID').agg(Revenue = ('Revenue', 'sum')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da84bd7e-8607-4fe7-ae88-21fad27d2a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = r_df.merge(f_df, on='CustomerID', how='inner').merge(m_df, on='CustomerID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c675e4-1f0e-4b2d-83c4-d8b213608b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
